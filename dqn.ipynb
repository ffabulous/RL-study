{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q-network\n",
    "\n",
    "- Q-learning becomes intractable when number of states is large\n",
    "- Train a Network that learns to map $state \\to (action, reward)$\n",
    "- However, Q-network diverges due to\n",
    "  - Correlations between samples (too similar)\n",
    "  - Non-stationary targets (gradient descent affects $Y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DQN\n",
    "\n",
    "- Use deep neural networks\n",
    "- Experience replay ([NIPS'13](https://arxiv.org/pdf/1312.5602))\n",
    "  - Store samples in memory and random sample\n",
    "- Separate action/target networks ([Nature'15](http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf))\n",
    "  - Fix target network and copy action network for N steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Equations\n",
    "\n",
    "\\begin{align*}\n",
    "Y = r_t + \\gamma\\underset{a'}\\max{\\hat{Q}_\\theta(s_{t+1}, a';\\bar\\theta)} \\\\\n",
    "\\hat{Y} = \\hat{Q}(S_t,a_t;\\theta) \\\\\n",
    "\\underset\\theta\\min\\sum_{t=0}^T[\\hat{Y}-Y]^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "1. Initialize replay memory $D$, action network $Q(\\theta)$, target network $\\hat{Q}(\\bar\\theta)$\n",
    "2. Do Forever:\n",
    "  - Select $a_t = \\text{arg}\\underset{a}\\max Q(s_t,a;\\theta)$ with probability $1-\\epsilon$\n",
    "  - Excute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$\n",
    "  - Store transition $(s_t, a_t, r_t, s_{t+1}) \\to D$\n",
    "  - Sample random mini-batch from $D$\n",
    "  - Set $y_t = r_t$ if terminates else $r_t + \\gamma\\underset{a'}\\max{\\hat{Q}_\\theta(s_{t+1}, a';\\bar\\theta)}$\n",
    "  - Perform gradient descent step on $(y_t-Q(s_t, a_t;\\theta))^2$\n",
    "  - Copy $\\hat{Q}\\gets Q$ every $C$ steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-13 18:41:57,433] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# Setup gym\n",
    "env = gym.make('CartPole-v0')\n",
    "# env = gym.wrappers.Monitor(env, directory='gym-results/', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "HIDDEN_SIZE = 16\n",
    "GAMMA = 0.99\n",
    "REPLAYS = 10000\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_BY = 5\n",
    "EPISODES = 500\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DQN\n",
    "D = deque(maxlen=REPLAYS)    # replay buffer\n",
    "Q = nn.Sequential(\n",
    "    nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE),\n",
    ")\n",
    "Q_bar = copy.deepcopy(Q)    # target network\n",
    "optimizer = optim.Adam(Q.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "for episode in range(EPISODES):\n",
    "    epsilon = 1 / ((episode / 10) + 1)\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    step = 1\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:    # exploit & explore\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q(Variable(torch.FloatTensor(state)).unsqueeze(0)))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward = -1.0    # penalty\n",
    "        D.append((state, action, reward, next_state, done))\n",
    "        if len(D) > BATCH_SIZE:    # SGD\n",
    "            batch = random.sample(D, BATCH_SIZE)\n",
    "            states = Variable(torch.FloatTensor(map(itemgetter(0), batch)))\n",
    "            actions = Variable(torch.LongTensor(map(itemgetter(1), batch)))\n",
    "            rewards = Variable(torch.FloatTensor(map(itemgetter(2), batch)))\n",
    "            next_states = Variable(torch.FloatTensor(map(itemgetter(3), batch)))\n",
    "            terminates = Variable(torch.FloatTensor(map(itemgetter(4), batch)))    # true=1\n",
    "            optimizer.zero_grad()\n",
    "            y = rewards + GAMMA * Q_bar(next_states).detach().max(dim=1)[0] * (1-terminates)    # detach not to backprop\n",
    "            y_h = Q(states).gather(1, actions.unsqueeze(1))\n",
    "            loss = criterion(y_h, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        if step % UPDATE_BY == 0:\n",
    "            Q_bar = copy.deepcopy(Q)\n",
    "        step += 1\n",
    "        state = next_state\n",
    "    print episode, step, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "state = env.reset()\n",
    "rewards = 0\n",
    "while True:\n",
    "    env.render()\n",
    "    action = np.argmax(Q(Variable(torch.FloatTensor(state)).unsqueeze(0)))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    rewards += reward\n",
    "    if done:\n",
    "        print rewards\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
