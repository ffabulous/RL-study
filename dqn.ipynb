{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q-network\n",
    "\n",
    "- Q-learning becomes intractable when number of states is large\n",
    "- Train a Network that learns to map $state \\to (action, reward)$\n",
    "- However, Q-network diverges due to\n",
    "  - Correlations between samples (too similar)\n",
    "  - Non-stationary targets (gradient descent affects $Y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DQN\n",
    "\n",
    "- Use deep neural networks\n",
    "- Experience replay ([NIPS'13](https://arxiv.org/pdf/1312.5602))\n",
    "  - Store samples in memory and random sample\n",
    "- Separate action/target networks ([Nature'15](http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf))\n",
    "  - Fix target network and copy action network for N steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Equations\n",
    "\n",
    "\\begin{align*}\n",
    "Y = r_t + \\gamma\\underset{a'}\\max{\\hat{Q}_\\theta(s_{t+1}, a';\\bar\\theta)} \\\\\n",
    "\\hat{Y} = \\hat{Q}(S_t,a_t;\\theta) \\\\\n",
    "\\underset\\theta\\min\\sum_{t=0}^T[\\hat{Y}-Y]^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Algorithm\n",
    "\n",
    "1. Initialize replay memory $D$, action network $Q(\\theta)$, target network $\\hat{Q}(\\bar\\theta)$\n",
    "2. Do Forever:\n",
    "  - Select $a_t = \\text{arg}\\underset{a}\\max Q(s_t,a;\\theta)$ with probability $1-\\epsilon$\n",
    "  - Excute action $a_t$ and observe reward $r_t$ and next state $s_{t+1}$\n",
    "  - Store transition $(s_t, a_t, r_t, s_{t+1}) \\to D$\n",
    "  - Sample random mini-batch from $D$\n",
    "  - Set $y_t = r_t$ if terminates else $r_t + \\gamma\\underset{a'}\\max{\\hat{Q}_\\theta(s_{t+1}, a';\\bar\\theta)}$\n",
    "  - Perform gradient descent step on $(y_t-Q(s_t, a_t;\\theta))^2$\n",
    "  - Copy $\\hat{Q}\\gets Q$ every $C$ steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-14 15:45:15,624] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "# Setup gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "HIDDEN_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "REPLAYS = 10000\n",
    "BATCH_SIZE = 64\n",
    "UPDATE_BY = 5\n",
    "EPISODES = 100\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DQN\n",
    "D = deque(maxlen=REPLAYS)    # replay buffer\n",
    "Q = nn.Sequential(\n",
    "    nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE),\n",
    ")\n",
    "Q_bar = copy.deepcopy(Q)    # target network\n",
    "optimizer = optim.Adam(Q.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 17, 120, 17, 17, 70, 139, 43, 45, 144, 30, 51, 117, 133, 173, 200, 139, 200, 186, 200, 111, 162, 200, 74, 200, 161, 200, 166, 191, 200, 200, 197, 200, 197, 200, 200, 200, 198, 11, 186, 196, 200, 200, 200, 200, 181, 182, 200, 200, 178, 200, 200, 200, 179, 186, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 191, 200, 200, 200, 200, 200, 189, 196, 200, 200, 195, 188, 200, 200, 200, 200, 200, 198, 200, 200, 200, 181, 191, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "steps = []\n",
    "for episode in range(EPISODES):\n",
    "    epsilon = 1 / ((episode / 10) + 1)\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:    # exploit & explore\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            _state = Variable(torch.FloatTensor(state)).unsqueeze(0)\n",
    "            action = np.argmax(Q(_state).data.numpy())\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward = -1.0    # penalty\n",
    "        D.append((state, action, reward, next_state, done))\n",
    "        if len(D) > BATCH_SIZE:    # SGD\n",
    "            batch = random.sample(D, BATCH_SIZE)\n",
    "            states = Variable(torch.FloatTensor(map(itemgetter(0), batch)))\n",
    "            actions = Variable(torch.LongTensor(map(itemgetter(1), batch)))\n",
    "            rewards = Variable(torch.FloatTensor(map(itemgetter(2), batch)))\n",
    "            next_states = Variable(torch.FloatTensor(map(itemgetter(3), batch)))\n",
    "            terminates = Variable(torch.FloatTensor(map(itemgetter(4), batch)))    # true=1\n",
    "            optimizer.zero_grad()\n",
    "            y = rewards + GAMMA * Q_bar(next_states).detach().max(dim=1)[0] * (1-terminates)    # detach not to backprop\n",
    "            y_h = Q(states).gather(1, actions.unsqueeze(1))\n",
    "            loss = criterion(y_h, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        if step % UPDATE_BY == 0:\n",
    "            Q_bar = copy.deepcopy(Q)\n",
    "        step += 1\n",
    "        state = next_state\n",
    "    steps.append(step)\n",
    "print steps    # max=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-14 15:47:38,230] Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "[2017-06-14 15:47:38,233] Starting new video recorder writing to gym-results/openaigym.video.0.12296.video000000.mp4\n",
      "[2017-06-14 15:47:45,661] Starting new video recorder writing to gym-results/openaigym.video.0.12296.video000001.mp4\n",
      "[2017-06-14 15:48:12,391] Starting new video recorder writing to gym-results/openaigym.video.0.12296.video000008.mp4\n",
      "[2017-06-14 15:49:19,305] Starting new video recorder writing to gym-results/openaigym.video.0.12296.video000027.mp4\n",
      "[2017-06-14 15:51:26,287] Starting new video recorder writing to gym-results/openaigym.video.0.12296.video000064.mp4\n",
      "[2017-06-14 15:53:30,049] Finished writing results. You can upload them to the scoreboard via gym.upload('gym-results')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "env = gym.wrappers.Monitor(env, directory='gym-results/', force=True)\n",
    "rewards = []\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        _state = Variable(torch.FloatTensor(state)).unsqueeze(0)\n",
    "        action = np.argmax(Q(_state).data.numpy())\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        step += reward\n",
    "        if done:\n",
    "            rewards.append(step)\n",
    "            break\n",
    "print np.mean(rewards)\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-14 15:53:43,066] [CartPole-v0] Uploading 100 episodes of training data\n",
      "[2017-06-14 15:53:45,227] [CartPole-v0] Uploading videos of 5 training episodes (53574 bytes)\n",
      "[2017-06-14 15:53:46,013] [CartPole-v0] Creating evaluation object from gym-results/ with learning curve and training video\n",
      "[2017-06-14 15:53:46,626] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on CartPole-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_ta7gMWo3R4aZdMBEmZjeqA\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "gym.upload('gym-results/', api_key='open_ai_gym_key')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
